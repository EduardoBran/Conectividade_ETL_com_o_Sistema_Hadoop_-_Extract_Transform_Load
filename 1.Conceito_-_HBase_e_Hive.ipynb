{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c316ec91",
   "metadata": {},
   "source": [
    "# <center><span style=\"font-size: 42px;color: darkgreen;\">Conceito Sobre <u>HBase</u> e <u>Hive</u></center></span>\n",
    "\n",
    "Até aqui, estudamos o **Hadoop HDFS**, que é um sistema de armazenamento distribuído, e o **Hadoop MapReduce**, uma estrutura de processamento de dados em grandes volumes.\n",
    "\n",
    "Agora, avançaremos no `ecossistema do Hadoop` para explorar dois dos componentes mais utilizados: o **HBase** e o **Hive**. Ambos `rodam sobre o HDFS`, de modo que ainda continuaremos a estudar aspectos do próprio HDFS e MapReduce, pois esses componentes os utilizam em algum momento.\n",
    "\n",
    "- **HBase** é um banco de dados **NoSQL** não relacional, ideal para armazenar grandes quantidades de dados não estruturados e fornecer acesso rápido a dados para operações de leitura e escrita.\n",
    "- **Hive** é um banco de dados relacional que armazena dados no HDFS, permitindo organizá-los em formato de tabela e manipulá-los com `SQL`. Diferentemente do trabalho direto via linha de comando, o Hive oferece uma interface mais acessível e funcionalidades adicionais, como a consulta e o armazenamento de dados no HDFS usando uma linguagem familiar.\n",
    "\n",
    "Esses novos componentes expandem nossas possibilidades de manipulação e análise de dados, com interfaces e ferramentas mais robustas que se integram diretamente ao HDFS e MapReduce.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5943ddc5",
   "metadata": {},
   "source": [
    "# <center><u><span style=\"font-size: 38px;color: darkgreen;\">Apache HBase</span></center></u>\n",
    "\n",
    "\n",
    "### O que é?\n",
    "\n",
    "O `HBase` é um banco de dados **não relacional** e **orientado a colunas**, projetado para lidar com grandes volumes de dados distribuídos em clusters. Ele faz parte da categoria de bancos de dados não relacionais, dos quais existem mais de *60 tipos*, divididos em *4 categorias principais*.\n",
    "\n",
    "Atualmente, o `MongoDB` lidera o mercado de bancos de dados não relacionais, sendo amplamente utilizado para gerenciar dados **não estruturados** ou **semi-estruturados**. Esses bancos são ideais para dados que não seguem uma estrutura rígida.\n",
    "\n",
    "Para **dados estruturados**, utilizamos bancos de dados relacionais, como o `Oracle`, `SQL Server` ou o próprio `**Apache Hive**`, que é adequado para análise de dados em **ambientes Hadoop**.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Porque o HBase é considerado banco de dados não relacional orientado a colunas ?\n",
    "\n",
    "O `HBase` é considerado um banco de dados **não relacional orientado a colunas** por causa de sua estrutura de armazenamento e organização dos dados, que diferem bastante dos bancos de dados relacionais convencionais. Aqui estão as principais razões:\n",
    "\n",
    "- **Estrutura Flexível**: Não possui esquemas rígidos como bancos relacionais, permitindo que cada linha tenha colunas diferentes, ideal para dados não estruturados ou semi-estruturados.\n",
    "- **Orientação a Colunas**: Armazena dados em **famílias de colunas**, permitindo acessar apenas as colunas necessárias, o que otimiza consultas em grandes volumes de dados.\n",
    "- **Escalabilidade com HDFS**: Funciona sobre o **Hadoop Distributed File System (HDFS)**, tornando-o escalável e adequado para grandes volumes de dados distribuídos.\n",
    "- **Sem SQL Nativo**: Não usa SQL nativo, mas permite operações de leitura e escrita por uma API própria. O **Apache Phoenix** pode ser usado para consultas SQL sobre o HBase.\n",
    "\n",
    "Esses fatores fazem do HBase uma boa escolha para grandes volumes de dados distribuídos, oferecendo flexibilidade e eficiência.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Comandos Básicos do HBase\n",
    "\n",
    "<br>\n",
    "\n",
    "Acessar o shell do hbase com o comando: `hbase shell`.\n",
    "\n",
    "- **Exibe a versão do HBase**: `version`\n",
    "- **Mostra o status do HBase**: `status`\n",
    "- **Lista todos os comandos disponíveis**: `help`\n",
    "- **Exibe ajuda relacionada a tabelas**: `table_help`\n",
    "- **Exibe o usuário logado**: `whoami`\n",
    "- **Exibe um status simples**: `status 'simple'`\n",
    "- **Exibe um status resumido**: `status 'summary`\n",
    "- **Exibe um status detalhado**: `status 'detailed'`\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Comandos Para Operações em Tabelas\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Criar uma tabela**: `create 'dsacademy', {NAME => 'ALUNO'}, {NAME => 'INSTRUTOR'}`\n",
    "- **Visualizar a estrutura de uma tabela**: `describe 'dsacademy'`\n",
    "- **Listar todas as tabelas**: `list`\n",
    "- **Verificar se uma tabela existe**: `exists 'table1'`\n",
    "- **Deletar uma tabela**: `disable 'dsacademy'`, `drop 'dsacademy'`\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Principais Características do HBase\n",
    "\n",
    "<br>\n",
    "\n",
    "O HBase oferece uma série de características que o tornam uma escolha popular para ambientes de Big Data e sistemas distribuídos:\n",
    "\n",
    "- **Escalabilidade Horizontal**: O HBase permite a adição de novos servidores para expandir a capacidade de armazenamento e processamento sem grandes mudanças na estrutura do banco de dados.\n",
    "- **Consistência e Disponibilidade**: Utilizando o conceito de “eventual consistency”, o HBase garante que as leituras e escritas sejam altamente consistentes, mesmo em grandes volumes de dados.\n",
    "- **Baixa Latência**: Por ser um banco orientado a colunas, o HBase é otimizado para leituras rápidas em grandes conjuntos de dados, oferecendo baixa latência em consultas específicas.\n",
    "- **Compatibilidade com HDFS**: Como o HBase é construído sobre o Hadoop Distributed File System (HDFS), ele se beneficia da escalabilidade e resiliência do HDFS para armazenamento de dados distribuídos.\n",
    "- **Suporte a Operações CRUD**: O HBase permite operações básicas de criação, leitura, atualização e exclusão (CRUD) em grandes volumes de dados, com suporte a transações básicas.\n",
    "- **Time Stamping**: O HBase mantém múltiplas versões de uma célula (com valores diferentes ao longo do tempo) usando carimbos de data e hora, o que é útil para rastrear mudanças nos dados.\n",
    "\n",
    "Essas características fazem do HBase uma ferramenta poderosa para aplicações de alta demanda que exigem desempenho e escalabilidade.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Apache HBase x Apache HDFS\n",
    "\n",
    "<br>\n",
    "\n",
    "Embora o HBase e o HDFS sejam parte do ecossistema Hadoop, eles servem a propósitos diferentes:\n",
    "\n",
    "- **HDFS**: O Hadoop Distributed File System (HDFS) é um sistema de arquivos distribuído projetado para armazenar grandes volumes de dados de forma redundante em clusters. É ideal para **armazenamento de dados em larga escala** e é a base de armazenamento para muitos sistemas Hadoop.\n",
    "- **HBase**: Já o HBase é um banco de dados que utiliza o HDFS como seu sistema de armazenamento subjacente, mas com a adição de uma camada de estrutura que permite consultas e operações rápidas. Enquanto o HDFS é ideal para leitura sequencial de dados, o HBase é mais adequado para **acesso aleatório e em tempo real**.\n",
    "\n",
    "Em resumo, o HDFS é para **armazenamento em larga escala**, enquanto o HBase é para **armazenamento estruturado com acesso em tempo real**, aproveitando a capacidade de escalabilidade do HDFS.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Apache HBase x Banco de Dados Relacionais\n",
    "\n",
    "<br>\n",
    "\n",
    "O HBase difere dos bancos de dados relacionais em vários aspectos:\n",
    "\n",
    "- **Esquema Flexível vs. Esquema Rígido**: O HBase permite que cada linha tenha colunas diferentes, enquanto em bancos de dados relacionais todas as linhas seguem o mesmo esquema fixo.\n",
    "- **Orientação a Colunas**: Diferente dos bancos relacionais (orientados a linhas), o HBase é **orientado a colunas**, o que permite otimizações específicas para grandes volumes de dados e acessos frequentes a um conjunto limitado de colunas.\n",
    "- **Escalabilidade**: O HBase foi projetado para escalar horizontalmente, adicionando servidores conforme o volume de dados aumenta. Já bancos de dados relacionais muitas vezes precisam de escalabilidade vertical (aumento de capacidade em um único servidor).\n",
    "- **Suporte a ACID**: Bancos de dados relacionais geralmente oferecem suporte completo a ACID (Atomicidade, Consistência, Isolamento e Durabilidade), enquanto o HBase se concentra em consistência eventual para otimizar a velocidade e escalabilidade.\n",
    "- **SQL vs. API**: O HBase não usa SQL nativo, mas possui uma API própria para manipulação de dados. O `Apache Phoenix` pode ser usado como uma camada de SQL para consultas, mas não é uma solução SQL completa como nos bancos de dados relacionais.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Afinal, Quando Usar e Quando Não Usar o HBase ?\n",
    "\n",
    "<br>\n",
    "\n",
    "#### <u>Quando Usar o HBase</u>:\n",
    "\n",
    "- **Grandes Volumes de Dados**: O HBase é ideal para armazenamento e acesso rápido a grandes volumes de dados distribuídos.\n",
    "- **Consultas em Tempo Real**: É uma ótima escolha quando se necessita de leitura e escrita em tempo real, como logs de atividades ou dados de redes sociais.\n",
    "- **Aplicações com Dados Semi-estruturados**: Quando os dados não possuem um esquema rígido e podem mudar com frequência, o HBase oferece flexibilidade para lidar com diferentes tipos de colunas por linha.\n",
    "- **Escalabilidade Horizontal**: Quando é necessário expandir o armazenamento e o processamento conforme o crescimento dos dados, o HBase oferece uma solução escalável.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### <u>Quando Não Usar o HBase</u>:\n",
    "\n",
    "- **Requisitos ACID Rígidos**: Se a aplicação exige transações ACID rigorosas, um banco de dados relacional é mais adequado.\n",
    "- **Consultas Complexas**: Para consultas complexas ou agregações envolvendo muitas tabelas e relacionamentos, bancos de dados relacionais são mais eficientes.\n",
    "- **Pequenas Bases de Dados**: O HBase é projetado para grandes volumes de dados. Para bases de dados pequenas, um banco relacional é mais eficiente e fácil de gerenciar.\n",
    "- **Necessidade de SQL Completo**: Se o SQL completo é um requisito (sem dependências de APIs adicionais como Apache Phoenix), um banco de dados relacional é mais apropriado.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Normalização e Armazenamento de Dados no HBase\n",
    "\n",
    "<br>\n",
    "\n",
    "Uma diferença essencial entre o HBase e os bancos de dados relacionais está na abordagem da **normalização**. Nos bancos relacionais, a normalização busca evitar redundância e \"mistura de assuntos\" em uma mesma tabela, garantindo que cada tabela represente um único tópico ou entidade.\n",
    "\n",
    "O HBase, porém, segue uma filosofia diferente. Em vez de aplicar normalização rigorosa, o HBase incentiva o uso de **famílias de colunas**, permitindo que dados relacionados sejam agrupados em uma única tabela. Essa abordagem aceita uma certa redundância e desnormalização para facilitar o acesso rápido e escalável a grandes volumes de dados.\n",
    "\n",
    "Esse modelo é ideal para ambientes de Big Data, onde a prioridade é o desempenho e a eficiência no armazenamento e acesso aos dados, em vez de evitar a duplicação de informações.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Orientação a Linha x Orientação a Coluna\n",
    "\n",
    "<br>\n",
    "\n",
    "A orientação de armazenamento de dados — *seja por linhas ou colunas* — é uma característica fundamental que define a arquitetura e o desempenho dos bancos de dados, especialmente em aplicações de grande escala e ambientes de Big Data. Essa orientação afeta como os dados são organizados, armazenados e acessados, impactando diretamente o desempenho de consultas e o uso de recursos.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Bancos de Dados Orientados a Linhas\n",
    "\n",
    "Nos **bancos de dados orientados a linhas**, os dados são armazenados **linha por linha**, ou seja, todos os valores de uma linha (ou registro) são armazenados juntos. Esse tipo de orientação é comum em bancos de dados relacionais tradicionais, como: `MySQL`, `PostgreSQL`, `SQL Server` ou `Oracle Database`.\n",
    "\n",
    "Esses bancos de dados são otimizados para operações que envolvem a **leitura e escrita de registros completos (ou linhas inteiras)**, o que os torna ideais para aplicações transacionais (OLTP) onde o objetivo é acessar registros inteiros de maneira rápida e consistente. Por exemplo, ao consultar dados de um cliente em um sistema de vendas, é comum buscar todas as informações de uma só vez (nome, endereço, telefone, etc.), o que é eficiente em bancos de dados orientados a linhas.\n",
    "\n",
    "#### <u>Vantagens dos Bancos Orientados a Linhas</u>:\n",
    "\n",
    "- **Desempenho em operações de inserção e atualização**: Atualizar ou inserir linhas inteiras é eficiente.\n",
    "- **Adequação para aplicações OLTP**: Transações rápidas com consultas que acessam registros inteiros.\n",
    "- **Modelo relacional e suporte a ACID**: A maioria dos bancos orientados a linhas oferece suporte completo a ACID, sendo adequado para transações seguras.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Bancos de Dados Orientados a Colunas\n",
    "\n",
    "Nos **bancos de dados orientados a colunas**, os dados são armazenados **coluna por coluna**. Em vez de armazenar todos os valores de uma linha juntos, o banco armazena os valores de cada coluna separadamente. Essa orientação é comum em bancos de dados `NoSQL` e em alguns sistemas de data warehousing, como: `Apache HBase`, `Apache Cassandra`, `Google Bigtable`, ou `Amazon Redshift `.\n",
    "\n",
    "Bancos orientados a colunas são otimizados para consultas analíticas (OLAP), em que o objetivo é **acessar e processar grandes volumes de dados em colunas específicas, como somas, contagens ou médias**. Em um cenário de análise de vendas, por exemplo, onde apenas uma ou duas colunas (como \"valor da venda\" e \"data\") são necessárias para análise, bancos de dados orientados a colunas podem processar essas consultas com grande eficiência.\n",
    "\n",
    "#### <u>Vantagens dos Bancos Orientados a Colunas</u>:\n",
    "\n",
    "- **Desempenho em consultas analíticas**: Permite que consultas analíticas sejam executadas de forma muito rápida ao acessar apenas as colunas necessárias.\n",
    "- **Compactação e armazenamento eficiente**: Armazenar dados por coluna permite compactação mais eficiente, economizando espaço e aumentando a velocidade das consultas.\n",
    "- **Ideal para OLAP**: Excelente para data warehousing e análise de dados.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Comparação: Quando Usar Cada Tipo\n",
    "\n",
    "- **Usar bancos orientados a linhas** quando a aplicação exige muitas operações de leitura e escrita de registros completos, como em sistemas transacionais e aplicações OLTP.\n",
    "- **Usar bancos orientados a colunas** quando o foco está em análises que acessam e processam grandes volumes de dados específicos, como em data warehousing e aplicações OLAP.\n",
    "\n",
    "Em resumo, a escolha entre `orientação a linha` e `orientação a coluna` depende do caso de uso e das necessidades da aplicação. A **orientação a linha é mais comum em bancos de dados transacionais**, enquanto a **orientação a coluna é altamente eficiente para análises e aplicações de Big Data**.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# <center><u><span style=\"font-size: 34px;color: darkgreen;\">Trabalhando com o HBase</span></center></u>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# <u>Lab 1</u> - Importando Dados para o `Apache HBase` com o `Apache Pig`\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1. Iniciando os Serviços\n",
    "\n",
    "<br>\n",
    "\n",
    "- **1.1 Iniciar o HDFS (NameNode, DataNode, SecondaryNameNode)**:\n",
    "   ```bash\n",
    "   start-dfs.sh  |  stop-dfs.sh\n",
    "   ```\n",
    "- **1.2 Iniciar o YARN (ResourceManager, NodeManager)**:\n",
    "   ```bash\n",
    "   start-yarn.sh  |  stop-yarn.sh\n",
    "   ```\n",
    "- **1.3 Iniciar o ZooKeeper (QuorumPeerMain)**:\n",
    "   ```bash\n",
    "   zkServer.sh start  |  zkServer.sh stop\n",
    "   ```\n",
    "- **1.4 Iniciar o HBase (HMaster, HRegionServer)**:\n",
    "   ```bash\n",
    "   start-hbase.sh  |  stop-hbase.sh\n",
    "   ```\n",
    "- **1.5 Iniciar o Job History Server (JobHistoryServer)**:\n",
    "   ```bash\n",
    "   mapred --daemon start historyserver | mapred --daemon stop historyserver\n",
    "   ```\n",
    "- **1.6 Verificando serviços**:\n",
    "   ```bash\n",
    "   jps\n",
    "   ```\n",
    "<br> <br> \n",
    "\n",
    "###  2. Carregando um Conjunto de Dados Para o `HDFS` (necessário apenas para etapa 4.1)\n",
    "\n",
    "- **2.1** Criar um novo diretório no **HDFS**: `hdfs dfs -mkdir /clientes`\n",
    "    - Ir no diretório do arquivo e digitar: `hdfs dfs -copyFromLocal clientes.txt /clientes`\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3. Criando Tabela no `HBase`\n",
    "\n",
    "<br>\n",
    "\n",
    "- **3.1** Ir no terminal e digitar: `hbase shell`\n",
    "  - Criar uma tabela com: `create 'clientes', 'dados_clientes'`\n",
    "  - Lista todas as tabelas: `list`\n",
    "  \n",
    "<br>\n",
    "\n",
    "### 4. Carregando Dados Para a Tabela no `Apache HBase` com o `Apache Pig`\n",
    "\n",
    "<br>\n",
    "\n",
    "**4.1 Do `HDFS` para o `HBase`**:  <span style=\"color: red;\"><strong><u>ATENÇÃO</u> NÃO ESTÁ FUNCIONANDO (Máquina Virtual Hadoop)</strong></span>\n",
    "\n",
    "<br>\n",
    "\n",
    "- **4.1.1** Iniciando o *shell* do **Apache Pig**: `pig -x mapreduce`\n",
    "  - Verificar o local com: `pwd` e digitar: `cd ..` para navegar até o **diretório com os *dados carregados*** no `HDFS`.\n",
    "  \n",
    "<br>\n",
    "\n",
    "- **4.1.2** Defininindo **estrutura** do arquivo *clientes.txt* carregado no `HDFS`:\n",
    "    ```bash\n",
    "dados = LOAD 'clientes.txt' USING PigStorage(',') AS (\n",
    "           id:chararray,\n",
    "           nome:chararray,\n",
    "           sobrenome:chararray,\n",
    "           idade:int,\n",
    "           funcao:chararray\n",
    ");\n",
    "    ```\n",
    "<br>\n",
    "\n",
    "- **4.1.3** Teste os dados: `dump dados;`\n",
    "\n",
    "<br>\n",
    "\n",
    "- **4.1.4** Usando Pig Store para **colocar os dados no `HBase`**:\n",
    "    ```bash\n",
    "STORE dados INTO 'hbase://clientes' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage(\n",
    "'dados_clientes:nome \n",
    " dados_clientes:sobrenome \n",
    " dados_clientes:idade \n",
    " dados_clientes:funcao'\n",
    ");\n",
    "    ```\n",
    "    \n",
    "  - A coluna `id` não é colocada aqui pois ela será usada como **row key** (equivalente a chave primária). O `Pig` identificará e colocará automaticamente como identificador no `HBase`.\n",
    "    \n",
    "<br><br>\n",
    "\n",
    "**4.2 Do `Sistema Local` para o `HBase`**: <span style=\"color: blue;\"><strong><u>ATENÇÃO</u> ESSE FUNCIONA</strong></span>\n",
    "\n",
    "<br>\n",
    "\n",
    "- **4.2.1** Iniciando o *shell* do **Apache Pig**: `pig -x local`\n",
    "  - Verificar o local com: `ls` e digitar: `cat nome_do_arquivo.txt` para conferir.\n",
    "  \n",
    "<br>\n",
    "\n",
    "- **4.2.2** Defininindo **estrutura** do arquivo *clientes.txt* carregado no `Sistema Local`:\n",
    "    ```bash\n",
    "dados = LOAD 'clientes.txt' USING PigStorage(',') AS (\n",
    "           id:chararray,\n",
    "           nome:chararray,\n",
    "           sobrenome:chararray,\n",
    "           idade:int,\n",
    "           funcao:chararray\n",
    ");\n",
    "    ```\n",
    "<br>\n",
    "\n",
    "- **4.2.3** Teste os dados: `dump dados;`\n",
    "\n",
    "<br>\n",
    "\n",
    "- **4.2.4** Usando Pig Store para **colocar os dados no `HBase`**:\n",
    "    ```bash\n",
    "STORE dados INTO 'hbase://clientes' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage(\n",
    "'dados_clientes:nome \n",
    " dados_clientes:sobrenome \n",
    " dados_clientes:idade \n",
    " dados_clientes:funcao'\n",
    ");\n",
    "    ```\n",
    "    \n",
    "  - A coluna `id` não é colocada aqui pois ela será usada como **row key** (equivalente a chave primária). O `Pig` identificará e colocará automaticamente como identificador no `HBase`.\n",
    "\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### 4. Manipulando os Dados Carregados com o `Apache HBase`\n",
    "\n",
    "<br>\n",
    "\n",
    "- **4.1** Ir no terminal e digitar: `hbase shell`\n",
    "- **4.2** Scan da Tabela: `scan 'clientes'`\n",
    "- **4.3** Count da Tabela: `count 'clientes'`\n",
    "- **4.4** Selecionando o nome de um único registro: `get 'clientes', '100002', {COLUMN => 'dados_clientes:nome'}`\n",
    "- **4.5** Editando o valor da coluna nome com base no id: `put 'clientes', '100002', 'dados_clientes:nome', 'Bob'`\n",
    "- **4.5** Adicionando uma nova linha:\n",
    "    ```bash\n",
    "put 'clientes', '2100002', 'dados_clientes:nome', 'Zico'\n",
    "put 'clientes', '2100002', 'dados_clientes:sobrenome', 'Galinho'\n",
    "put 'clientes', '2100002', 'dados_clientes:idade', '50'\n",
    "put 'clientes', '2100002', 'dados_clientes:funcao', 'Jogador'\n",
    "    ```\n",
    "- **4.5** Deletando uma coluna específica (idade) de uma linha usando id: `delete 'clientes', '2100002', 'dados_clientes:idade'`\n",
    "- **4.6** Deletando uma linha usando id: `deleteall 'clientes', '2100002'`\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "# <u>Lab 2</u> - Pipeline de Dados no HBase com API Java\n",
    "\n",
    "<br>\n",
    "\n",
    "### Importante\n",
    "\n",
    "O Ambiente usado será a <u>**`Máquina Virtual Cloudera 5.13`**</u>.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Passo a Passo\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. Abra a Aplicação Eclipse\n",
    "\n",
    "- **1.1** No ambiente Cloudera, abra o **Eclipse**.\n",
    "- **1.2** No menu superior, selecione **File > New > Java Project**.\n",
    "- **1.3** Dê um nome ao projeto (por exemplo, `Pipeline`) e clique em **Next**, em seguida em **Finish**. O novo projeto aparecerá na barra lateral esquerda.\n",
    "\n",
    "#### 2. Crie um Pacote para Organizar o Código\n",
    "\n",
    "- **2.1** Expanda o projeto recém-criado (`Pipeline`) clicando sobre ele na barra lateral.\n",
    "- **2.2** Clique com o botão direito na pasta **src** e selecione **New > Package**.\n",
    "- **2.3** Nomeie o pacote como `pipeop` e clique em **Finish**.\n",
    "\n",
    "#### 3. Adicione Classes Java para Operações no HBase\n",
    "\n",
    "- **3.1** Clique com o botão direito sobre o pacote `pipeop` que você acabou de criar.\n",
    "- **3.2** Selecione **New > Class**.\n",
    "- **3.3** Nomeie a classe como `Hbase_create` e clique em **Finish**.\n",
    "- **3.4** Abra o arquivo `Hbase_create.java` e cole o conteúdo fornecido no curso para essa classe.\n",
    "- **3.5** Repita os passos acima para as demais classes necessárias (`Hbase_insert`, `Hbase_read`, `ListTables`, etc.), colando o código fornecido em cada arquivo correspondente.\n",
    "\n",
    "#### 4. Configure o Caminho para as Bibliotecas e APIs Necessárias\n",
    "\n",
    "Para que o projeto funcione corretamente, você precisa incluir os arquivos `.jar` que contêm as bibliotecas e APIs Java necessárias para trabalhar com o Hadoop e o HBase. Siga os passos abaixo para configurar o **Build Path** no Eclipse:\n",
    "\n",
    "- **4.1** Clique com o botão direito no nome do projeto (`Pipeline`) e selecione **Build Path > Configure Build Path...**\n",
    "- **4.2** Na janela de configuração, selecione Add External JARs...\n",
    "- **4.3** Navegue até o diretório onde os arquivos .jar estão localizados e adicione-os ao projeto:\n",
    "\n",
    "  - **Hadoop**: Acesse `/usr/lib/hadoop/`, selecione todos os arquivos `.jar` e clique em **OK**.\n",
    "  - **MapReduce**: Acesse `/usr/lib/hadoop-0.20-mapreduce/`, selecione todos os arquivos `.jar` e clique em **OK**.\n",
    "  - **HDFS**: Acesse `/usr/lib/hadoop-hdfs/`, selecione todos os arquivos `.jar` e clique em **OK**.\n",
    "  - **HBase**: Acesse `/usr/lib/hbase/`, selecione todos os arquivos `.jar` e clique em **OK**.\n",
    "  - **HBase Lib**: Acesse `/usr/lib/hbase/lib/`, selecione todos os arquivos `.jar` e clique em **OK**.\n",
    "- **4.4** Após adicionar todos os `.jar`, clique em **Apply and Close** para salvar as configurações.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Resumo\n",
    "\n",
    "Neste laboratório, você criou um projeto chamado `Pipeline` no **Eclipse** na **Máquina Virtual Cloudera 5.13**. Dentro desse projeto, foi criado o pacote pipeop para organizar as classes Java necessárias. Essas classes (`Hbase_create`, `Hbase_insert`, `Hbase_read`, `ListTables`, etc.) são responsáveis por realizar operações no HBase usando a **API Java**. Para que o projeto funcione corretamente, foram configurados os caminhos para as bibliotecas `.jar`, importando os recursos necessários do Hadoop e HBase.\n",
    "\n",
    "Essa estrutura permite que o projeto `Pipeline` faça chamadas à API Java do HBase e utilize as bibliotecas Hadoop, garantindo o funcionamento do pipeline de dados no HBase.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## 5. Iniciando a Execução\n",
    "\n",
    "Agora que o projeto está configurado e as **classes Java** foram criadas, vamos iniciar a execução do pipeline de dados no **HBase**. Siga os passos abaixo para **criar uma tabela**, **inserir dados** e **verificar o conteúdo no HBase**.\n",
    "\n",
    "#### 5.1 Inicie o Shell do HBase\n",
    "\n",
    "- Abra o terminal e execute o comando para acessar o shell do HBase: `hbase shell`\n",
    "\n",
    "#### 5.2 Execute a Classe Hbase_create.java\n",
    "\n",
    "- Retorne ao **Eclipse**.\n",
    "- Na barra lateral, localize a classe `Hbase_create.java` no pacote `pipeop`.\n",
    "- Clique com o botão direito na classe `Hbase_create.java` e selecione **Run As > Java Application**.\n",
    "- Após a execução, verifique se a mensagem \"**Tabela criada com sucesso**\" é exibida no console do **Eclipse**.\n",
    "\n",
    "#### 5.3 Verifique a Tabela Criada no HBase\n",
    "\n",
    "- Volte para o terminal onde o shell do **HBase** está aberto.\n",
    "- Execute o comando abaixo para listar as tabelas e confirmar a criação da tabela RH: `list`\n",
    "\n",
    "#### 5.4 Execute a Classe `Hbase_insert.java` para Inserir Dados\n",
    "\n",
    "- No **Eclipse**, localize a classe `Hbase_insert.java` no pacote `pipeop`.\n",
    "- Clique com o *botão direito* na classe `Hbase_insert.java` e selecione **Run As > Java Application**.\n",
    "- Verifique se a mensagem \"**Dados inseridos**\" é exibida no console do Eclipse, indicando que os dados foram inseridos com sucesso.\n",
    "\n",
    "#### 5.5 Verifique os Dados Inseridos no HBase\n",
    "\n",
    "- Retorne ao terminal com o shell do **HBase**.\n",
    "- Execute os comandos abaixo para verificar os dados na tabela RH:\n",
    "  - Contar Registros: `count 'RH'`\n",
    "  - Exibir Registros: `scan 'RH'`\n",
    "\n",
    "Esses passos permitirão confirmar a criação da tabela e a inserção de dados no HBase, garantindo que o pipeline de dados está funcionando corretamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3da69",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "# <center><u><span style=\"font-size: 38px;color: darkgreen;\">Apache Hive</center></u></span>\n",
    "\n",
    "<br>\n",
    "\n",
    "O `Hive` é uma ferramenta de *data warehousing* que facilita a consulta e análise de grandes conjuntos de dados armazenados no **Hadoop Distributed File System (HDFS)**, utilizando uma linguagem semelhante ao SQL chamada `HiveQL`. Ele foi desenvolvido para tornar o Hadoop mais acessível a analistas e engenheiros de dados que estão acostumados com bancos de dados relacionais, mas precisam lidar com dados massivos distribuídos em clusters.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Por que o Hive é considerado adequado para análise de dados em ambientes Hadoop?\n",
    "\n",
    "O `Hive` é amplamente utilizado para consultas e agregações de dados, especialmente quando se trabalha com grandes volumes de dados estruturados e semi-estruturados em um ambiente Hadoop. Aqui estão algumas razões pelas quais o Hive é adequado para essa tarefa:\n",
    "\n",
    "- **Integração com HDFS**: O Hive foi projetado para trabalhar sobre o HDFS, permitindo que consultas SQL-like sejam executadas em grandes volumes de dados distribuídos.\n",
    "- **Schema on Read**: Diferentemente de bancos de dados tradicionais que aplicam o esquema durante a gravação (`Schema on Write`), o Hive aplica o esquema durante a leitura, oferecendo flexibilidade ao ingerir dados de diferentes fontes e formatos.\n",
    "- **Linguagem Familiar (HiveQL)**: O Hive usa uma linguagem semelhante ao SQL, chamada **HiveQL**, que permite que usuários familiarizados com SQL consultem dados sem precisar aprender linguagens complexas de programação.\n",
    "- **Compatível com Ferramentas de BI**: Muitas ferramentas de BI e ETL se integram bem com o Hive, facilitando a criação de relatórios e dashboards diretamente dos dados no Hadoop.\n",
    "- **Suporte a Processamento em Lote**: Embora não seja ideal para consultas em tempo real, o Hive é eficaz para **processamento em lote**, ideal para análises que requerem agregações de grandes conjuntos de dados.\n",
    "\n",
    "Essas características fazem do Hive uma solução popular para análise de dados em ambientes Hadoop, permitindo que as organizações tirem proveito de dados distribuídos de maneira eficiente e escalável.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Quando Usar o Apache Hive?\n",
    "\n",
    "Usamos o `Apache Hive` quando precisamos realizar consultas ou manipulações em grandes conjuntos de dados, tais como seleção de registros ou colunas, agregação, sumarização, contagem de elementos, filtros ou atualizações em massa.\n",
    "\n",
    "**Essas tarefas não precisam ser feitas em tempo real** e o que queremo é obter insights a partir de grandes conjuntos de dados, Big Data.\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# <center><u><span style=\"font-size: 34px;color: darkgreen;\">Trabalhando com o Hive</span></center></u>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# <u>Lab 1</u> - Manipulação de Dados com o `Apache Hive`\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### 1. Iniciando os Serviços\n",
    "\n",
    "<br>\n",
    "\n",
    "- **1.1 Iniciar o HDFS (NameNode, DataNode, SecondaryNameNode)**:\n",
    "   ```bash\n",
    "   start-dfs.sh  |  stop-dfs.sh\n",
    "   ```\n",
    "- **1.2 Iniciar o YARN (ResourceManager, NodeManager)**:\n",
    "   ```bash\n",
    "   start-yarn.sh  |  stop-yarn.sh\n",
    "   ```\n",
    "- **1.3 Verificando serviços**:\n",
    "   ```bash\n",
    "   jps\n",
    "   ```\n",
    "<br>\n",
    "\n",
    "###  2. Iniciar o Shell do `Hive`\n",
    "\n",
    "<br>\n",
    "\n",
    "- **2.1 Acessar o console do Hive**:\n",
    "   ```bash\n",
    "   hive\n",
    "   ```\n",
    "- **2.2 Verificar bancos de dados existentes**:\n",
    "   ```bash\n",
    "   show databases;\n",
    "   ```\n",
    "<br>\n",
    "\n",
    "###  3. Criando Banco de Dados no `Hive`\n",
    "\n",
    "<br>\n",
    "\n",
    "- **3.1 Ainda no console do Hive, para criar um novo banco de dados (*`dsacademy`*)**:\n",
    "   ```bash\n",
    "   create database dsacademy;\n",
    "   ```\n",
    "- **3.2 Dizendo ao Hive que vamos usar/selecionar o novo banco de dados criado**:\n",
    "   ```bash\n",
    "   use dsacademy;\n",
    "   ```\n",
    "   \n",
    "<br>\n",
    "\n",
    "###  4. Manipulando Dados no Banco de Dados no `Hive`\n",
    "\n",
    "<br>\n",
    "\n",
    "- **4.1 Criando Tabela no novo Banco de Dados (*`dsacademy`*)**:\n",
    "  - Cria uma tabela chamada `colaboradores` para armazenar dados de funcionários com as colunas `id`, `nome`, `cargo`, e `salario`:\n",
    "   ```bash\n",
    "   CREATE TABLE IF NOT EXISTS colaboradores (id int, nome String, cargo String, salario decimal)\n",
    "COMMENT 'tabela de colaboradores'\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY '\\t'\n",
    "LINES TERMINATED BY '\\n';\n",
    "   ```\n",
    "- **4.2 Verificando existência de tabelas no novo Banco de Dados (*`dsacademy`*)**:\n",
    "   ```bash\n",
    "   show tables;\n",
    "   ```\n",
    "- **4.3 Verificando estrutura de uma tabela no novo Banco de Dados (*`dsacademy`*)**:\n",
    "   ```bash\n",
    "   describe colaboradores;\n",
    "   ```\n",
    "- **4.4 Realizando alteração no tipo de uma coluna (iremos alterar o tipo da coluna `salario` para `Double`**:\n",
    "   ```bash\n",
    "   ALTER TABLE colaboradores CHANGE salario salario Double;\n",
    "   ```\n",
    "- **4.5 Adicionando uma nova coluna a tabela com descrição**:\n",
    "   ```bash\n",
    "   ALTER TABLE colaboradores ADD COLUMNS (cidade String COMMENT 'Nome da Cidade');\n",
    "   ```\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**<span style=\"font-size: 20px;color: darkgreen;\"><u>IMPORTANTE</u></span>**: No momento o banco de dados e sua tabela foram **criados e armazenados no HDFS** uma vez que o `Apache Hive` está **configurado para utilizar o HDFS como sistema de armazenamento**.\n",
    "\n",
    "No entanto, é possível **alterar essa configuração** para que o `Apache Hive` **aponte para outros serviços**, como o `Amazon S3` (serviço em nuvem) ou até mesmo o `HBase`.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "###  5. Carregando Massa de Dados na Tabela do Banco de Dados\n",
    "\n",
    "Os comandos abaixo serão executados na tabela `colaboradores` do banco de dados `dsacademy`.\n",
    "\n",
    "Usaremos o dataset `colaboradores.csv` disponibilizado.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **5.1 Criando uma `tabela temporária` (stage area)**:\n",
    "   ```bash\n",
    "   create table temp_colab (texto String);\n",
    "   ```\n",
    "   ```bash\n",
    "   show tables;\n",
    "   ```\n",
    "\n",
    "> Recomenda-se que novos dados sejam inicialmente carregados em uma tabela temporária (*stage area*). Isso permite realizar manipulações e/ou filtragens antes de mover os dados para a tabela de destino final, garantindo uma maior qualidade e integridade dos dados inseridos.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **5.2 Carregando os dados de `colaboradores.csv` para a tabela temporária**:\n",
    "   ```bash\n",
    "   LOAD DATA LOCAL INPATH '/home/hadoop/Documents/Datasets/colaboradores.csv' OVERWRITE INTO TABLE temp_colab;\n",
    "   ```\n",
    "   ```bash\n",
    "   select * from temp_colab;\n",
    "   ```\n",
    "- **5.3 Realizando Manipulação de Dados na *Stage Area* e transferindo para a tabela `colaboradores`** :\n",
    "\n",
    "\n",
    "INSERT overwrite table colaboradores\n",
    "\n",
    "SELECT  \n",
    "\n",
    "  regexp_extract(texto, '^(?:([^,]*),?){1}', 1) ID,  \n",
    "  regexp_extract(texto, '^(?:([^,]*),?){2}', 1) nome,  \n",
    "  regexp_extract(texto, '^(?:([^,]*),?){3}', 1) cargo,\n",
    "  regexp_extract(texto, '^(?:([^,]*),?){4}', 1) salario,  \n",
    "  regexp_extract(texto, '^(?:([^,]*),?){5}', 1) cidade\n",
    "\n",
    "FROM temp_colab;\n",
    "\n",
    "\n",
    "> O **comando acima** manipula os dados na tabela temporária (`temp_colab`), que contém linhas de texto, e transfere-os para a tabela final `colaboradores`. Durante essa transferência, os dados são processados para extrair os valores individuais das colunas usando uma expressão regular que separa os campos com base em vírgulas (`,`).\n",
    "\n",
    "> **Este processo** garante que os dados brutos da `stage area` (tabela temporária) sejam corretamente formatados e inseridos nas colunas adequadas da tabela de destino.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **5.4 Verificando resultado**:\n",
    "   ```bash\n",
    "   select * from colaboradores;\n",
    "   ```\n",
    "\n",
    "<br>\n",
    "\n",
    "###  6. Realizando Agragações\n",
    "\n",
    "- **6.1 Criando um filtro para exibir a linha cujo id = 1002**:\n",
    "   ```bash\n",
    "   SELECT * FROM colaboradores WHERE Id = 1002;\n",
    "   ```\n",
    "- **6.2 Criando um filtro para exibir a linha cujo cujo salario >= 25000**:\n",
    "   ```bash\n",
    "   SELECT * FROM colaboradores WHERE salario >= 25000;\n",
    "   ```\n",
    "- **6.3 Criando um filtro para exibir a linha cujo cujo salario > 10000 e cidade = 'Natal'**:\n",
    "   ```bash\n",
    "   SELECT * FROM colaboradores WHERE salario > 10000 AND cidade = 'Natal';\n",
    "   ```\n",
    "\n",
    "<br>\n",
    "\n",
    "- **6.4 Somando os salários por cidade**:\n",
    "  - Esse comando calcula a soma dos salários de todos os colaboradores, agrupando-os por cidade:\n",
    "   ```bash\n",
    "   SELECT sum(salario), cidade FROM colaboradores GROUP BY cidade;\n",
    "   ```\n",
    "- **6.5 Calculando a média salarial por cargo**:\n",
    "  - Aqui, calculamos a média salarial para cada cargo registrado na tabela:\n",
    "   ```bash\n",
    "   SELECT AVG(salario), cargo FROM colaboradores GROUP BY cargo;\n",
    "   ```\n",
    "- **6.6 Contando o número de colaboradores por cidade**:\n",
    "  - Esse comando conta quantos colaboradores existem em cada cidade:\n",
    "   ```bash\n",
    "   SELECT COUNT(*), cidade FROM colaboradores GROUP BY cidade;\n",
    "   ```\n",
    "- **6.7 Encontrando o salário máximo por cidade**:\n",
    "  - Esse comando encontra o maior salário em cada cidade:\n",
    "   ```bash\n",
    "   SELECT MAX(salario), cidade FROM colaboradores GROUP BY cidade;\n",
    "   ```\n",
    "\n",
    "<br>\n",
    "\n",
    "###  6. Acessando Via Interface pelo Navegador\n",
    "\n",
    "<br>\n",
    "\n",
    "- Abrir o navegador e digitar `localhost:9870`.\n",
    "- Clicar na aba **Utilities** e acessar **Browse the file system**.\n",
    "- Navegar até o diretório **User < hive < warehouse < colaboradores.\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "# Fim!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d82d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
